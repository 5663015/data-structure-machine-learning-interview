# 《机器学习基础知识与面试题》目录

- 机器学习基础
  - 判别模型与生成模型的本质区别
  - 线性分类和非线性分类各有哪些模型
  - 机器学习中常用的损失函数有哪些？比较各模型的损失函数
  - 特征选择的方法
  - 介绍一个完整的机器学习项目流程
  - 总结各类机器学习算法的特点、优缺点
  - 常用的模型性能度量方法
  - 误差、方差和偏差的区别、联系
  - 解释k折交叉验证，k取值多少有什么关系
  - 什么是经验误差、泛化误差
  - 如何解决欠拟合、过拟合
  - 类别不平衡问题解决方法
  - 你会在时间序列数据集上使用什么交叉验证技术？是用k倍或LOOCV？
  - 给你一个数据集，这个数据集有缺失值，且这些缺失值分布在离中值有1个标准偏差的范围内。百分之多
    少的数据不会受到影响？为什么？
- 数学相关
  - 泰勒展开，在机器学习、深度学习中的应用
  - 协方差和相关系数有什么区别？
  - 范数
  - 特征分解
  - 最大似然估计和最大后验概率的区别?
  - 概率和似然的区别与联系
  - 常见的概率分布
  - KL散度、JS散度
  - 贝叶斯
  - 解释贝叶斯公式和朴素贝叶斯分类
  - 朴素贝叶斯分类器出现估计概率值为 0 怎么处理，各有什么优缺点
  - 贝叶斯分类器的优化和特殊情况的处理
- 线性回归
  - 线性回归原理，如何训练如何调参
  - 线性回归要求因变量服从正态分布？
  - Logistic回归
  - LR推导
  - LR如何处理多分类
  - 贝叶斯分类和LR的区别
  - LR是否可以用核函数
  - LR是否可以用来处理非线性问题
  - 为什么LR需要归一化或者取对数，为什么LR把特征离散化后效果更好
  - 为什么LR可以用来做CTR预估？
  - 逻辑回归的优缺点
  - 逻辑回归为什么要对特征进行离散化
  - 线性回归与逻辑回归的区别
  - 逻辑回归常用的优化算法
  - L1正则化产生稀疏性的原因
  - Lasso回归
  - 岭回归
- SVM
  - svm推导
  - 引入拉格朗日的优化方法的原因
  - svm里的参数 的作用
  - 解释原问题和对偶问题
  - KKT限制条件
  - 核函数的作用及常用的核函数
  - svm如何解决多分类问题
  - svm如何解决回归问题
  - SVM的主要特点
  - 加大训练数据量一定能提高SVM准确率吗
  - LR和SVM的区别和联系
  - svm与感知器、神经网络的联系和优缺点对比
  - 集成学习
  - Boosting
  - Bagging
  - boosting和bagging的区别
  - 从方差偏差角度解释bagging和boosting
  - 集成学习的结合策略
  - 什么是包外估计（OOB）
  - 集成学习是否一定会有提升
- 树模型
  - 决策树算法的优缺点
  - 有关熵的概念及理解
  - 如何理解信息增益
  - 基尼指数
  - ID3、C4.5和CART三种决策树的原理及其区别对比
  - ID3、C4.5、CART的优缺点
  - 决策树如何剪枝
  - 随机森林原理及优缺点
  - 随机森林的优缺点
  - 随机森林如何处理缺失值
  - 提升树算法
  - 梯度提升决策树GBDT算法过程
  - 梯度提升与梯度下降的区别
  - GBDT的优缺点
  - 随机森林与GBDT之间的区别与联系
  - XGBoost原理
  - 为什么XGBoost要用泰勒展开
  - XGBoost与GBDT的区别
  - XGBoost的内部优化
  - XGBoost如何调参
  - lightgbm原理
  - xgboost和lightgbm的区别
  - 决策树如何避免过拟合
- 降维
  - PCA原理
  - 线性判别分析LDA，与pca的区别
  - EM
  - EM原理
  - 采用 EM 算法求解的模型有哪些，为什么不用牛顿法或梯度下降法？
  - 高斯混合模型
- 聚类
  - kmeans 的原理，优缺点
  - Kmeans 算法 中K 怎么设置
  - 怎么评价 Kmeans 聚类结果
  - Kmeans算法改进
  - EM 与 kmeans 的关系
  - 如何判断自己实现的 LR、Kmeans 算法是否正确
  - 解释密度聚类算法
  - 聚类算法中的距离度量有哪些
  - DBSCAN原理和算法伪代码，与kmeans，OPTICS区别
- 强化学习
  - 马尔科夫决策过程
  - 策略迭代
  - 值迭代
  - 解释强化学习的策略梯度
- 深度学习
  - 激活函数及其选择
  - BP反向传播、随机梯度下降法权重更新公式
  - 卷积公式
  - 关于卷积层的特点、作用
  - 1x1卷积的作用
  - 如何减少卷积的参数
  - RNN原理
  - LSTM原理
  - 推导LSTM反向传播过程
  - 为什么会出现梯度消失或梯度爆炸，如何解决
  - 导致模型不收敛的原因
  - 解决深度学习过拟合
  - 深度学习中的正则化
  - 如何根据损失变化调整网络
  - 参数初始化策略
  - BN及其作用作用？
  - 如何理解Group Normalization？
  - VGG使用2个3x3卷积核的优势？
  - Sigmoid和tanh为什么会导致梯度消失
  - Relu相比于sigmoid、tanh的优点和局限性
  - Dropout流程
  - 解释空洞卷积原理
  - 如何理解转置卷积
  - 什么是分组卷积GroupConvolution
  - 什么是深度可分离卷积
  - embedding层的作用
  - attention机制
- 优化
  - 如何判断函数凸或非凸
  - 梯度下降的优缺点
  - 梯度下降法的原理以及各个变种
  - 如果有若干个极小值点，如何避免陷入局部最优解
  - 最速下降法和共轭梯度法
  - 理解全局最优与局部最优
- 参考文献  